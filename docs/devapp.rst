Developing an OP2 Application
=============================

This page provides a tutorial in the basics of using OP2 for unstructured-mesh application development.

Example Application
-------------------

The tutorial will use the Airfoil application, a simple non-linear  2D  inviscid  airfoil code that uses an unstructured mesh. It is a finite volume application that solves the 2D Euler equations using a scalar numerical dissipation. The algorithm iterates towards the steady state solution, in each iteration using a control volume approach - for example the rate at which the mass changes within a control volume is equal to the net flux of mass into the control volume across the four faces around the cell.

Airfoil consists of five loops, ``save_soln`` , ``adt_calc`` , ``res_calc`` , ``bres_calc`` and ``update``, within a time-marching iterative loop. Out of these, ``save_soln`` and ``update`` are what we classify as direct loops where all the data accessed in the loop is defined on the mesh element over which the loop iterates over. Thus for example in a direct loop a loop over edges will only access data defined on edges. The other three loops are indirect loops. In this case when looping over a given type of elements, data on other types of elements will be accessed indirectly, using mapping tables. Thus for example ``res_calc`` iterates over edges and increments data on cells, accessing them indirectly via a mapping table that gives the explicit connectivity information between edges and cells.

The standard mesh size solved with Airfoil consists of 1.5M edges.  Here the most compute intensive loop is ``res_calc``, which is called 2000 times during the total execution of the application and performs about 100 floating-point operations per mesh edge.

* Go to the ``OP2/apps/c/airfoil/airfoil/airfoil_tutorial/original`` directory and open the ``airfoil_orig.cpp`` file to view the original application.
* Use the Makefile in the same directory to compile and then run the application. The ``new_grid.dat`` (downloadable from `here <https://op-dsl.github.io/docs/OP2/new_grid.dat>`__) needs to be present in the same directory as the executable.
* The program executes by reporting the rms value of the pressure held on the cells for every 100 iterations and ends by comparing this value to the reference value of the computation at 1000 iterations. If the solution is within machine precision of the reference value, the execution is considered to be validated (PASS). This is the same criteria required to validate any parallel versions of the application, including the paralleizations generated by OP2 as detailed below.


Original - Load mesh and initialization
---------------------------------------
The original code begins with allocating memory to hold the mesh data and then initializing them by reading in the mesh data, form the ``new_grid.dat`` text file:


.. code-block:: C

  FILE *fp;
  if ((fp = fopen(FILE_NAME_PATH, "r")) == NULL) {
    printf("can't open file FILE_NAME_PATH\n");
    exit(-1);
  }
  if (fscanf(fp, "%d %d %d %d \n", &nnode, &ncell, &nedge, &nbedge) != 4) {
	printf("error reading from FILE_NAME_PATH\n");
	exit(-1);
  }

  cell   = (int *)malloc(4 * ncell  * sizeof(int));
  edge   = (int *)malloc(2 * nedge  * sizeof(int));
  ecell  = (int *)malloc(2 * nedge  * sizeof(int));
  bedge  = (int *)malloc(2 * nbedge * sizeof(int));
  becell = (int *)malloc(1 * nbedge * sizeof(int));
  bound  = (int *)malloc(1 * nbedge * sizeof(int));
  x      = (double *)malloc(2 * nnode * sizeof(double));
  q      = (double *)malloc(4 * ncell * sizeof(double));
  qold   = (double *)malloc(4 * ncell * sizeof(double));
  res    = (double *)malloc(4 * ncell * sizeof(double));
  adt    = (double *)malloc(1 * ncell * sizeof(double));

  for (int n = 0; n < nnode; n++) {
	if (fscanf(fp, "%lf %lf \n", &x[2 * n], &x[2 * n + 1]) != 2) {
  	  printf("error reading from FILE_NAME_PATH\n");
  	  exit(-1);
	}
  }

  for (int n = 0; n < ncell; n++) {
    if (fscanf(fp, "%d %d %d %d \n", &cell[4 * n], &cell[4 * n + 1],
        &cell[4 * n + 2], &cell[4 * n + 3]) != 4) {
      printf("error reading from FILE_NAME_PATH\n");
      exit(-1);
    }
  }
  for (int n = 0; n < nedge; n++) {
	if (fscanf(fp, "%d %d %d %d \n", &edge[2 * n], &edge[2 * n + 1],
        &ecell[2 * n], &ecell[2 * n + 1]) != 4) {
      printf("error reading from FILE_NAME_PATH\n");
  	  exit(-1);
    }
  }
  for (int n = 0; n < nbedge; n++) {
    if (fscanf(fp, "%d %d %d %d \n", &bedge[2 * n], &bedge[2 * n + 1],
        &becell[n], &bound[n]) != 4) {
    printf("error reading from FILE_NAME_PATH\n");
    exit(-1);
    }
  }
  fclose(fp);


The code then initialize ``q`` and ``res`` data arrays to 0.

Original - Main iteration and loops over mesh
---------------------------------------------

The main iterative loop is a for loop that iterates for some ``NUM_ITERATIONS`` which in this case is set to 1000 iterations.  Within this main iterative loops there are 5 loops over various mesh elements (as noted above) including direct and indirect loops.


Build OP2
---------
Build OP2 using instructions in the `Getting Started <fhttps://op2-dsl.readthedocs.io/en/latest/getting_started.html>`__. page.

Step 1 - Preparing to use OP2
-----------------------------

First, include the following header files, then initialize OP2 and finalize it as follows:

.. code-block:: C

  #include "op_seq.h"
  ...
  ...
  int main(int argc, char **argv) {
    //Initialise the OP2 library, passing runtime args, and setting diagnostics level to low (1)
    op_init(argc, argv, 1);
    ...
    ...
    ...
    free(adt);
    free(res);

    //Finalising the OP2 library
    op_exit();
  }

By this point you need OP2 set up - take a look at the Makefile in step1, and observe that the include and library paths are added, and we link against ``op2_seq`` back-end library.


Step 2 - OP2 Declaration
------------------------

**Declare sets** - The Airfoil application consists of four mesh element types (which we call sets): nodes, edges, cells and boundary edges. These needs to be declared using the ``op_set`` API call together with the number of elements for each of these sets:

.. code-block:: C

  // declare sets
  op_set nodes  = op_decl_set(nnode,  "nodes" );
  op_set edges  = op_decl_set(nedge,  "edges" );
  op_set bedges = op_decl_set(nbedge, "bedges");
  op_set cells  = op_decl_set(ncell,  "cells" );

Later, we will see how the number of mesh elements can be read in directly from an hdf5 file using the ``op_set_hdf5`` call.

When developing your own application with OP2, or indeed converting an application to use OP2, you will need to decide on what mesh element types, i.e. sets will need to be declared to define the full mesh. A good starting point for this design is to see what mesh elements are used the loops over the mesh.

**Declare maps** - Looking at the original Airfoil application's loops we see that mappings between edges and nodes, edges and cells, boundary edges and nodes, boundary edges and cells, and cells and nodes are required. This can be observed by the indirect access to data in each of the loops in the main iteration loops. These connectivity information needs to be declared via the ``op_decl_map`` API call:

.. code-block:: C

  //declare maps
  op_map pedge   = op_decl_map(edges,  nodes, 2, edge,   "pedge"  );
  op_map pecell  = op_decl_map(edges,  cells, 2, ecell,  "pecell" );
  op_map pbedge  = op_decl_map(bedges, nodes, 2, bedge,  "pbedge" );
  op_map pbecell = op_decl_map(bedges, cells, 1, becell, "pbecell");
  op_map pcell   = op_decl_map(cells,  nodes, 4, cell,   "pcell"  );

The ``op_decl_map`` requires the names of the two sets for which the mapping is declared, its arity, mapping data (as in this case allocated in integer blocks of memory) and a string name.

**Declare data** - All data declared on sets should be declared using the ``op_decl_dat`` API call. For Airfoil this consists of the mesh coordinates data ``x``, new and old solution ``q`` and ``q_old``, area time step ``adt``, flux residual ``res`` and boundary flag ``bound`` that indicates if the edge is a boundary edge:

.. code-block:: C

  //declare data on sets
  op_dat p_bound = op_decl_dat(bedges, 1, "int",    bound, "p_bound");
  op_dat p_x     = op_decl_dat(nodes,  2, "double", x,     "p_x"    );
  op_dat p_q     = op_decl_dat(cells,  4, "double", q,     "p_q"    );
  op_dat p_qold  = op_decl_dat(cells,  4, "double", qold,  "p_qold" );
  op_dat p_adt   = op_decl_dat(cells,  1, "double", adt,   "p_adt"  );
  op_dat p_res   = op_decl_dat(cells,  4, "double", res,   "p_res"  );

**Declare constants** - Finally global constants that are used in any of the computations in the loops needs to be declared. This is required due to the fact that when using code-generation later for parallelizations such as on GPUs (e.g. using CUDA), global constants needs to be copied over to the GPUs before they can be used in a GPU kernel. Declaring them using the ``op_decl_const`` API call will indicate to the OP2 code-generator that these constants needs to be handled in a special way, generating code for copying them to the GPU for the relevant back-ends.

.. code-block:: C

  //declare global constants
  op_decl_const(1, "double", &gam  );
  op_decl_const(1, "double", &gm1  );
  op_decl_const(1, "double", &cfl  );
  op_decl_const(1, "double", &eps  );
  op_decl_const(1, "double", &alpha);
  op_decl_const(4, "double", qinf  );

Finally information about the the declared mesh can be viewed using a diagnostics level of 2 in ``op_init`` and calling ``op_diagnostic_output()`` API call:

.. code-block:: C

  /* main application */
  int main(int argc, char **argv) {
  //Initialise the OP2 library, passing runtime args, and setting diagnostics level to low (1)
  op_init(argc, argv, 2);
  ...
  ... op_decl_set ...
  ... op_decl_map ...
  ... op_decl_dat ...
  ... op_decl_const  ...
  ...
  //output mesh information
  op_diagnostic_output();

Finally compile the step2 application and execute. You will note that the full application still runs and validates as OP2, with the sequential back-end simply uses the allocated memory for sets, maps and data in the declaration, without internally de-allocating them. This helps the developer to gradually build up the application with the conversion to OP2 API (as we are do here), checking for validation on each step. However, this will only work for this developer sequential version, where none of the parallel versions generated via the code generator nor the code generated sequential version ``gen_seq`` will work as they de-allocate the initial memory and move the mesh to obtain best parallel performance.


Step 3 - First parallel loop : direct loop
------------------------------------------

We can now convert the first loop to use the OP2 API. In this case its a direct loop called ``save_soln`` that iterates over cells and saves the previous time-iteration's solution, ``q`` to ``q_old``:

.. code-block:: C

  //save_soln : iterates over cells
  for (int iteration = 0; iteration < (ncell * 4); ++iteration) {
    qold[iteration] = q[iteration];
  }

This is a direct loops due to the fact that all data accessed in the computation are defined on the set that the loop iterates over. In this case the iteration set is cells.

To convert to the OP2 API we first outline the loop body (elemental kernel) to a subroutine:

.. code-block:: C

  //outlined elemental kernel
  inline void save_soln(double *q, double *qold) {
  for (int n = 0; n < 4; n++)
    qold[n] = q[n];
  }

  //save_soln : iterates over cells
  for (int iteration = 0; iteration < (ncell * 4); ++iteration) {
    save_soln(&q[iteration], &qold[iteration]);
  }

Now we can directly declare the loop with the ``op_par_loop`` API call:

.. code-block:: C

  op_par_loop(save_soln, "save_soln", cells,
              op_arg_dat(p_q,    -1, OP_ID, 4, "double", OP_READ ),
              op_arg_dat(p_qold, -1, OP_ID, 4, "double", OP_WRITE));

Note how we have:

- indicated the elemental kernel ``save_soln`` in the first argument to ``op_par_loop``
- used the ``op_dat``s names ``p_q`` and ``p_qold`` in the API call
- noted the iteration set ``cells`` (3rd argument)
- indicated the direct access of ``q`` and ``q_old`` using ``OP_ID``
- indicated that ``p_q`` is read only (``OP_READ``) and ``q_old`` is written to only (``OP_WRITE``), by looking through the elemental kernel and identifying how they are used/accessed in the kernel.
- given that ``p_q`` is read only we also indicate this by the key word ``const`` for ``save-soln`` elemental kernel.
- The fourth argument of an ``op_arg_dat`` is the dimension of the data. For ``p_q`` and ``p_qold`` there are 4 doubles per mesh point.

Compile and execute the modified application (see code in ``../step3``) and check if the solution validates.

Step 4 - Indirect loops
-----------------------

The next loop in the application ``adt_calc`` calculate area/timstep and iterates over cells. In this case we see that the loop is an indirect loop where the data ``x`` on the four nodes connected to a cell is accessed indirectly via a cells to nodes mapping. Additionally data ``adt`` are accessed directly where ``adt`` is data on the cells:

.. code-block:: C

  //adt_calc - calculate area/timstep : iterates over cells
  for (int iteration = 0; iteration < ncell; ++iteration) {
    int map1idx = cell[iteration * 4 + 0];
    int map2idx = cell[iteration * 4 + 1];
    int map3idx = cell[iteration * 4 + 2];
    int map4idx = cell[iteration * 4 + 3];

    double dx, dy, ri, u, v, c;

    ri = 1.0f / q[4 * iteration + 0];
    u = ri * q[4 * iteration + 1];
    v = ri * q[4 * iteration + 2];
    c = sqrt(gam * gm1 * (ri * q[4 * iteration + 3] - 0.5f * (u * u + v * v)));

    dx = x[2 * map2idx + 0] - x[2 * map1idx + 0];
    dy = x[2 * map2idx + 1] - x[2 * map1idx + 1];
    adt[iteration] = fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x[2 * map3idx + 0] - x[2 * map2idx + 0];
    dy = x[2 * map3idx + 1] - x[2 * map2idx + 1];
    adt[iteration] += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x[2 * map4idx + 0] - x[2 * map3idx + 0];
    dy = x[2 * map4idx + 1] - x[2 * map3idx + 1];
    adt[iteration] += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x[2 * map1idx + 0] - x[2 * map4idx + 0];
    dy = x[2 * map1idx + 1] - x[2 * map4idx + 1];
    adt[iteration] += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    adt[iteration] = (adt[iteration]) / cfl;
  }

Similar to the direct loop, we outline the loop body and call it within the loop as follows:

.. code-block:: C

  //outlined elemental kernel - adt_calc
  inline void adt_calc(double *x1, double *x2, double *x3,
                       double *x4, double *q, double *adt) {
    double dx, dy, ri, u, v, c;

    ri = 1.0f / q[0];
    u = ri * q[1];
    v = ri * q[2];
    c = sqrt(gam * gm1 * (ri * q[3] - 0.5f * (u * u + v * v)));

    dx = x2[0] - x1[0];
    dy = x2[1] - x1[1];
    *adt = fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x3[0] - x2[0];
    dy = x3[1] - x2[1];
    *adt += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x4[0] - x3[0];
    dy = x4[1] - x3[1];
    *adt += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    dx = x1[0] - x4[0];
    dy = x1[1] - x4[1];
    *adt += fabs(u * dy - v * dx) + c * sqrt(dx * dx + dy * dy);

    *adt = (*adt) / cfl;
  }

  //adt_calc - calculate area/timstep : iterates over cells
  for (int iteration = 0; iteration < ncell; ++iteration) {
    int map1idx = cell[iteration * 4 + 0];
    int map2idx = cell[iteration * 4 + 1];
    int map3idx = cell[iteration * 4 + 2];
    int map4idx = cell[iteration * 4 + 3];

    adt_calc(&x[2 * map1idx], &x[2 * map2idx], &x[2 * map3idx],
             &x[2 * map4idx], &q[4 * iteration], &adt[iteration]);
  }

Now, convert the loop to use the ``op_par_loop`` API:

.. code-block:: C

  //res_calc - calculate flux residual: iterates over edges
  op_par_loop(adt_calc, "adt_calc", cells,
              op_arg_dat(p_x,   0, pcell, 2, "double", OP_READ ),
              op_arg_dat(p_x,   1, pcell, 2, "double", OP_READ ),
              op_arg_dat(p_x,   2, pcell, 2, "double", OP_READ ),
              op_arg_dat(p_x,   3, pcell, 2, "double", OP_READ ),
              op_arg_dat(p_q,  -1, OP_ID, 4, "double", OP_READ ),
              op_arg_dat(p_adt,-1, OP_ID, 1, "double", OP_WRITE));

Note in this case how the indirections are specified using the mapping declared as OP2 map ``pcell``, indicating the to-set index (2nd argument), and access mode ``OP_READ``.


Step 5 - Global reductions
--------------------------
* Details of ``op_par_loop`` for specifying global reductions


Step 6 - Handing it all to OP2
------------------------------
* Now can run a sequential version and validate results
* Partitioning call for MPI
* Parallel file I/O
* details on ``op_fetch_data`` call

Step 7 - Code generation
------------------------
* Code-gen command
* Link and execute parallel versions with Makefiles
* Use OP2's c_app Makefiles

Code generated versions
-----------------------

Optimizations
-------------
* Brief notes on runtime and optimization flags
* Provide link to Performance tuning page in the docs
